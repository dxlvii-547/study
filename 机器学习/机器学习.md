## 监督学习与无监督学习

**监督学习(supervised learning)：** 给出部分x与y的联系，得到更多x与y的联系，类似于传统线性回归模型中建立x与y的映射关系 x可以是矩阵
**无监督学习(unsupervised learning)：** 仅输入x，由机器学习模型自行判断,自行分类，常用于聚类分析，或是异常检测
## 线性回归模型
cost function  损失函数/成本函数 
![[Pasted image 20231127184948.png]]
不同的损失函数代表了不同模型的特性
求解损失函数的最小值即得到估计值 minimize J(w,b)
## 梯度下降法
![[Pasted image 20231127192811.png]]
alpha为学习率
too small -> slow  
too large ->  overshoot   or  diverge 
达到局部最小值时 导数为零 w不再变动 ![[Pasted image 20231127193846.png]]
## 多维特征
从样本y^(i) -> 线性回归模型
X -> **X  
![[Pasted image 20231127194511.png]]
python 中可以运用numpy  numpy 主要用于大量维度数组和矩阵运算
![[Pasted image 20231127194727.png]]
np.dot需要给出x和w
如何判断梯度下降法的收敛性
![[Pasted image 20231127195916.png]]
可以用收敛检测
let epsilon be 10^-3
if J decreases by <= epsilon in one iteration, declare convergence 
那么 如何设定恰当的学习率  ->尝试  尝试一系列值 拟合J以观察
## 逻辑回归
“binary classification”
0.5 -> 示性函数
![[Pasted image 20231127201749.png]]
其中，sigmoid函数可以视作将全域映射至(0,1)
逻辑回归的损失函数 用以寻找决策边界 
![[Pasted image 20231127205343.png]]
and
![[Pasted image 20231127205432.png]]
and
![[Pasted image 20231127205620.png]]
## 决策树
利用最大影响特征进行结点划分
衡量标准为熵
熵表示随机变量不确定性的度量 
![[Pasted image 20231130151335.png]]
~~~  为什么选用log？
随机变量可以讨论概率，此处随机变量的概率代表了不同特征分类结果的可能性 ，其值域为[0,1], 而熵表示混乱程度，其值域为[0,++8),log恰好可以建立两者的映射关系，同时，log可以对数据进行一定程度的压缩处理
信息增益来确定结点
~~~
**信息增益： 特征X使Y不确定性减少的程度
信息增益：熵-条件熵（一个条件下，信息不确定性的减少程度）
信息增益率：信息增益/条件的信息熵
**  
举例：
一、熵：
![[Pasted image 20231130193634.png]]
二、条件熵：
![[Pasted image 20231130193727.png]]
三、信息增益率
![[Pasted image 20231130193924.png]]
![[Pasted image 20231130194900.png]]
1. ID3：考虑信息增益。假设存在ID字段，ID信息增益极大，但无意义
2. C4.5 ：考虑信息增益率。同样ID字段，此时大分子大分母，但分母更大
3. CART：使用GINI系数：1-sigma_k=1,2,...K (p^2_k)； 

** 决策树剪枝 
1. 为什么要剪枝：过拟合风险，理论上数据可以完全隔离，即一个叶子节点一个数据 

> 为什么叶子结点多会过拟合？
-- 特征过多代表模型对训练集剖析得越彻底，达到极高的拟合程度，同时也大幅降低了模型的泛化能力 
![[Pasted image 20231130200533.png]]

2. 策略：  -- 预剪枝   --后剪枝 
3.  -- 预剪枝 ： 边建立树边剪
具体实施可以考虑限制深度，叶子节点个数，叶子节点样本数，信息增益量等
4.   -- 后剪枝 ： 建立后再剪枝
![[Pasted image 20231130205623.png]]
决策树总而言之是一种从根节点开始一步步走到叶子结点的决策模型，所有的数据最终都会落到叶子节点，既可以做分类也可以做回归 
> 